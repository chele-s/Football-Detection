{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1521e5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture install_output\n",
    "\n",
    "import sys\n",
    "print(\"üîÑ Instalando dependencias... (esto puede tomar 3-7 minutos)\")\n",
    "print(f\"Python: {sys.version}\")\n",
    "\n",
    "# Paso 1: Limpiar instalaciones previas de MMDetection\n",
    "print(\"\\nüßπ Limpiando instalaciones previas de MMDetection...\")\n",
    "!pip uninstall -y mmcv mmcv-full mmdet mmengine 2>/dev/null || true\n",
    "!pip cache purge 2>/dev/null || true\n",
    "\n",
    "# Paso 2: Actualizar herramientas b√°sicas\n",
    "print(\"üì¶ Actualizando herramientas base...\")\n",
    "%pip install -q --upgrade pip setuptools wheel\n",
    "\n",
    "# Paso 3: Instalar Ultralytics para YOLO\n",
    "print(\"üì¶ Instalando Ultralytics (YOLO)...\")\n",
    "%pip install -q ultralytics\n",
    "\n",
    "# Paso 4: Instalar dependencias comunes\n",
    "print(\"üì¶ Instalando dependencias comunes...\")\n",
    "%pip install -q pycocotools pyyaml tqdm tabulate colorama seaborn\n",
    "\n",
    "# Paso 5: Detectar Python 3.12\n",
    "from packaging.version import Version\n",
    "py_ver = Version(f\"{sys.version_info.major}.{sys.version_info.minor}\")\n",
    "SKIP_MMDET = py_ver >= Version(\"3.12\")\n",
    "\n",
    "if SKIP_MMDET:\n",
    "    print(\"\\n‚ö†Ô∏è  Python 3.12+ detectado. Omitiendo MMDetection.\")\n",
    "    print(\"    Recomendaci√≥n: Cambiar a Python 3.10 para soporte completo.\")\n",
    "    print(\"    El notebook ejecutar√° YOLOv8/YOLOv9 √∫nicamente.\")\n",
    "else:\n",
    "    # Paso 6: Instalar MMDetection con versiones EXACTAS y compatibles\n",
    "    print(\"\\nüì¶ Instalando MMDetection (versiones compatibles)...\")\n",
    "    %pip install -q openmim\n",
    "\n",
    "    # Instalar en orden y con versiones exactas\n",
    "    print(\"  ‚Üí mmengine 0.10.4...\")\n",
    "    !mim install -q --no-cache-dir mmengine==0.10.4\n",
    "\n",
    "    print(\"  ‚Üí mmcv 2.1.0...\")\n",
    "    !mim install -q --no-cache-dir mmcv==2.1.0\n",
    "\n",
    "    print(\"  ‚Üí mmdet 3.3.0...\")\n",
    "    !mim install -q --no-cache-dir mmdet==3.3.0\n",
    "\n",
    "    # Verificaci√≥n post-instalaci√≥n de versiones\n",
    "    print(\"\\nüîç Verificando versiones instaladas de MMDetection...\")\n",
    "    try:\n",
    "        import mmengine\n",
    "        print(f\"  mmengine: {mmengine.__version__}\")\n",
    "    except:\n",
    "        print(\"  mmengine: ‚úó Error\")\n",
    "\n",
    "    try:\n",
    "        import mmcv\n",
    "        print(f\"  mmcv: {mmcv.__version__}\")\n",
    "    except:\n",
    "        print(\"  mmcv: ‚úó Error\")\n",
    "\n",
    "    try:\n",
    "        import mmdet\n",
    "        print(f\"  mmdet: {mmdet.__version__}\")\n",
    "    except:\n",
    "        print(\"  mmdet: ‚úó Error (puede necesitar reinicio)\")\n",
    "\n",
    "# Paso 7: Sanity-check de imports clave\n",
    "print(\"\\nüîç Verificando instalaciones de m√≥dulos principales...\")\n",
    "import importlib\n",
    "mods = [\"ultralytics\",\"pycocotools\",\"yaml\",\"tqdm\",\"tabulate\",\"colorama\",\"seaborn\"]\n",
    "if not SKIP_MMDET:\n",
    "    mods.extend([\"mmengine\",\"mmcv\",\"mmdet\"])\n",
    "\n",
    "all_ok = True\n",
    "for m in mods:\n",
    "    try:\n",
    "        importlib.import_module(m)\n",
    "        print(f\"‚úì ok: {m}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚úó error_import: {m}: {str(e)[:100]}\")\n",
    "        all_ok = False\n",
    "\n",
    "if all_ok:\n",
    "    print(\"\\n‚úÖ Todas las instalaciones verificadas correctamente\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  Algunos m√≥dulos fallaron. Puede requerirse reinicio del runtime.\")\n",
    "\n",
    "print(\"\\n‚úÖ Fin de instalaci√≥n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d92d9cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analizar log de instalaci√≥n de forma robusta e inteligente\n",
    "stdout = getattr(install_output, 'stdout', '') or ''\n",
    "stderr = getattr(install_output, 'stderr', '') or ''\n",
    "combined = stdout + \"\\n\" + stderr\n",
    "combined_lower = combined.lower()\n",
    "\n",
    "# Contar m√≥dulos verificados\n",
    "ok_count = combined.count('‚úì ok:')\n",
    "error_count = combined.count('‚úó error_import:')\n",
    "\n",
    "# Detectar si MMDetection fue omitido\n",
    "mmdet_skipped = 'python 3.12' in combined_lower and 'omitiendo' in combined_lower\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"üìä RESUMEN DE INSTALACI√ìN\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "if mmdet_skipped:\n",
    "    print(\"\\n‚ö†Ô∏è  MMDetection OMITIDO (Python 3.12 detectado)\")\n",
    "    print(\"   ‚Üí El notebook ejecutar√° SOLO YOLOv8 y YOLOv9\")\n",
    "    print(\"   ‚Üí Para incluir Faster R-CNN: usar Python 3.10 o 3.11\\n\")\n",
    "    expected_modules = 7  # sin mmengine, mmcv, mmdet\n",
    "else:\n",
    "    expected_modules = 10  # con mmengine, mmcv, mmdet\n",
    "\n",
    "print(f\"‚úì M√≥dulos importados correctamente: {ok_count}/{expected_modules}\")\n",
    "if error_count > 0:\n",
    "    print(f\"‚úó M√≥dulos con errores de importaci√≥n: {error_count}\")\n",
    "\n",
    "# Mostrar m√≥dulos OK\n",
    "ok_lines = [line.strip() for line in combined.splitlines() if '‚úì ok:' in line]\n",
    "if ok_lines:\n",
    "    print(f\"\\nüì¶ M√≥dulos verificados:\")\n",
    "    for line in ok_lines:\n",
    "        module_name = line.split('ok:')[-1].strip()\n",
    "        print(f\"   ‚Ä¢ {module_name}\")\n",
    "\n",
    "# Mostrar errores cr√≠ticos\n",
    "error_lines = [line.strip() for line in combined.splitlines() if '‚úó error_import:' in line]\n",
    "if error_lines:\n",
    "    print(f\"\\n‚ùå Errores de importaci√≥n detectados:\")\n",
    "    for line in error_lines:\n",
    "        print(f\"   {line}\")\n",
    "\n",
    "# Decisi√≥n final\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "\n",
    "if error_count == 0:\n",
    "    print(\"‚úÖ ESTADO: LISTO PARA CONTINUAR\")\n",
    "    print(\"=\"*70)\n",
    "    print(\"\\nüöÄ Puedes ejecutar la siguiente celda (Importaciones)\")\n",
    "\n",
    "elif mmdet_skipped and error_count == 0:\n",
    "    print(\"‚úÖ ESTADO: LISTO (SIN MMDETECTION)\")\n",
    "    print(\"=\"*70)\n",
    "    print(\"\\nüöÄ Puedes continuar. Se ejecutar√°n YOLOv8 y YOLOv9 √∫nicamente\")\n",
    "\n",
    "elif 'mmdet' in str(error_lines).lower() and error_count <= 3:\n",
    "    print(\"‚ö†Ô∏è  ESTADO: REQUIERE REINICIO DE RUNTIME\")\n",
    "    print(\"=\"*70)\n",
    "    print(\"\\nüîÑ ACCI√ìN REQUERIDA:\")\n",
    "    print(\"   1. Ve a: Entorno de ejecuci√≥n ‚Üí Reiniciar entorno de ejecuci√≥n\")\n",
    "    print(\"   2. Despu√©s del reinicio, ejecuta SOLO la celda de Importaciones\")\n",
    "    print(\"   3. Si mmdet importa correctamente, contin√∫a con el experimento\")\n",
    "    print(\"\\nÔøΩ Nota: MMDetection requiere reinicio tras la instalaci√≥n inicial\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ùå ESTADO: ERRORES CR√çTICOS\")\n",
    "    print(\"=\"*70)\n",
    "    print(\"\\n‚ö†Ô∏è  ACCI√ìN REQUERIDA:\")\n",
    "    print(\"   1. Reinicia el runtime completamente\")\n",
    "    print(\"   2. Vuelve a ejecutar la celda de instalaci√≥n\")\n",
    "    print(\"   3. Si el problema persiste, verifica la versi√≥n de Python (3.10-3.11 recomendado)\")\n",
    "\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3520c1bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "from pathlib import Path\n",
    "from types import SimpleNamespace\n",
    "from datetime import datetime\n",
    "\n",
    "IN_COLAB = (\"google.colab\" in sys.modules) or (os.environ.get(\"COLAB_GPU\") is not None)\n",
    "\n",
    "default_base = Path(\"/content/Football_dataset\") if IN_COLAB else (Path.cwd() / \"data\" / \"Football_dataset\")\n",
    "BASE_PATH = Path(os.environ.get(\"DATA_DIR\", str(default_base))).resolve()\n",
    "BASE_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "OUTPUTS_DIR = BASE_PATH / \"outputs\"\n",
    "LOGS_DIR = OUTPUTS_DIR / \"logs\"\n",
    "OUTPUTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "LOGS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "experiment_name = f\"yolo_football_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "EPOCHS = int(os.environ.get(\"EPOCHS\", 100))\n",
    "IMG_SIZE = int(os.environ.get(\"IMG_SIZE\", 640))\n",
    "BATCH_SIZE = int(os.environ.get(\"BATCH_SIZE\", 16))\n",
    "\n",
    "config = SimpleNamespace(\n",
    "    IN_COLAB=IN_COLAB,\n",
    "    BASE_PATH=str(BASE_PATH),\n",
    "    OUTPUTS_DIR=str(OUTPUTS_DIR),\n",
    "    LOGS_DIR=str(LOGS_DIR),\n",
    "    experiment_name=experiment_name,\n",
    "    EPOCHS=EPOCHS,\n",
    "    IMG_SIZE=IMG_SIZE,\n",
    "    BATCH_SIZE=BATCH_SIZE,\n",
    "    DATASET_DIR=None,\n",
    "    DATA_YAML=None,\n",
    ")\n",
    "\n",
    "print(\"BASE_PATH:\", config.BASE_PATH)\n",
    "print(\"OUTPUTS_DIR:\", config.OUTPUTS_DIR)\n",
    "print(\"LOGS_DIR:\", config.LOGS_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba1fab1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q roboflow\n",
    "import os, sys, shutil, requests, zipfile, io\n",
    "from urllib.parse import urlparse, urlunparse, urlencode, parse_qsl\n",
    "from pathlib import Path\n",
    "from roboflow import Roboflow\n",
    "\n",
    "IN_COLAB = (\"google.colab\" in sys.modules) or (os.environ.get(\"COLAB_GPU\") is not None)\n",
    "\n",
    "default_base = Path(\"/content/Football_dataset\") if IN_COLAB else (Path.cwd() / \"data\" / \"Football_dataset\")\n",
    "base_fallback = Path(os.environ.get(\"DATA_DIR\", str(default_base))).resolve()\n",
    "base_fallback.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "try:\n",
    "    BASE_PATH = Path(config.BASE_PATH).resolve() if 'config' in globals() else base_fallback\n",
    "except Exception:\n",
    "    BASE_PATH = base_fallback\n",
    "\n",
    "API_KEY = os.environ.get(\"ROBOFLOW_API_KEY\") or os.environ.get(\"RF_API_KEY\") or \"\"\n",
    "WORKSPACE = os.environ.get(\"ROBOFLOW_WORKSPACE\", \"ecosort-onbc8\")\n",
    "PROJECT = os.environ.get(\"ROBOFLOW_PROJECT\", \"football-ball-ufsgy-f4vq2\")\n",
    "VERSION = int(os.environ.get(\"ROBOFLOW_VERSION\", \"1\"))\n",
    "UNIVERSE_URL = os.environ.get(\"ROBOFLOW_UNIVERSE_URL\", \"\")  # opcional, pega aqu√≠ tu URL de Universe\n",
    "\n",
    "if not API_KEY:\n",
    "    print(\"‚ö†Ô∏è  ROBOFLOW_API_KEY no est√° definida. Config√∫rala y vuelve a ejecutar esta celda:\")\n",
    "    print(\"   %env ROBOFLOW_API_KEY=TU_API_KEY\")\n",
    "\n",
    "print(\"Descargando dataset desde Roboflow...\")\n",
    "print(f\"  Workspace: {WORKSPACE}\")\n",
    "print(f\"  Project: {PROJECT}\")\n",
    "print(f\"  Version: {VERSION}\")\n",
    "print(f\"  Destino: {BASE_PATH}\")\n",
    "\n",
    "\n",
    "def find_data_yaml_in(roots):\n",
    "    roots = [Path(r) for r in roots if Path(r).exists()]\n",
    "    for r in roots:\n",
    "        for root, dirs, files in os.walk(str(r)):\n",
    "            if \"data.yaml\" in files:\n",
    "                return Path(root) / \"data.yaml\"\n",
    "    return None\n",
    "\n",
    "\n",
    "def build_universe_candidates(workspace: str, project: str, version: int, api_key: str, user_url: str):\n",
    "    cands = []\n",
    "    if user_url:\n",
    "        try:\n",
    "            pu = urlparse(user_url)\n",
    "            path = pu.path.rstrip('/')\n",
    "            if path.endswith(f\"/dataset/{version}\"):\n",
    "                dl = path + \"/download\"\n",
    "                q = dict(parse_qsl(pu.query))\n",
    "                q.update({\"api_key\": api_key, \"format\": \"yolov8\"})\n",
    "                cands.append(urlunparse((pu.scheme, pu.netloc, dl, '', urlencode(q), '')))\n",
    "            # Siempre probamos con format param por si ya trae /download\n",
    "            q = dict(parse_qsl(pu.query))\n",
    "            q.update({\"api_key\": api_key, \"format\": \"yolov8\"})\n",
    "            cands.append(urlunparse((pu.scheme, pu.netloc, path, '', urlencode(q), '')))\n",
    "        except Exception:\n",
    "            pass\n",
    "    cands.extend([\n",
    "        f\"https://universe.roboflow.com/{workspace}/{project}/dataset/{version}/download?api_key={api_key}&format=yolov8\",\n",
    "        f\"https://universe.roboflow.com/{workspace}/{project}/dataset/{version}?api_key={api_key}&format=yolov8\",\n",
    "        f\"https://universe.roboflow.com/{workspace}/{project}/dataset/yolov8/{version}?api_key={api_key}\",\n",
    "    ])\n",
    "    # Eliminar duplicados preservando orden\n",
    "    seen = set()\n",
    "    out = []\n",
    "    for u in cands:\n",
    "        if u and u not in seen:\n",
    "            out.append(u)\n",
    "            seen.add(u)\n",
    "    return out\n",
    "\n",
    "\n",
    "def try_download_zip(url: str, dest_dir: Path) -> bool:\n",
    "    try:\n",
    "        print(f\"  Probar: {url}\")\n",
    "        r = requests.get(url, stream=True, allow_redirects=True, timeout=180)\n",
    "        if r.status_code >= 400:\n",
    "            print(f\"   ‚Üí HTTP {r.status_code}\")\n",
    "            return False\n",
    "        content = r.content if not r.raw else r.content\n",
    "        try:\n",
    "            with zipfile.ZipFile(io.BytesIO(content)) as zf:\n",
    "                zf.extractall(dest_dir)\n",
    "            print(f\"   ‚úì Extra√≠do en {dest_dir}\")\n",
    "            return True\n",
    "        except zipfile.BadZipFile:\n",
    "            # Intentar escribir a archivo y extraer por si es grande\n",
    "            tmp_zip = dest_dir / \"rf_tmp.zip\"\n",
    "            with open(tmp_zip, 'wb') as f:\n",
    "                for chunk in r.iter_content(chunk_size=1024*1024):\n",
    "                    if chunk:\n",
    "                        f.write(chunk)\n",
    "            try:\n",
    "                with zipfile.ZipFile(tmp_zip, 'r') as zf:\n",
    "                    zf.extractall(dest_dir)\n",
    "                print(f\"   ‚úì Extra√≠do en {dest_dir}\")\n",
    "                return True\n",
    "            finally:\n",
    "                try: tmp_zip.unlink()\n",
    "                except Exception: pass\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚úó Error: {e}\")\n",
    "        return False\n",
    "\n",
    "try:\n",
    "    ok = False\n",
    "\n",
    "    # 1) SDK Roboflow\n",
    "    if API_KEY:\n",
    "        try:\n",
    "            rf = Roboflow(api_key=API_KEY)\n",
    "            project = rf.workspace(WORKSPACE).project(PROJECT)\n",
    "            dataset = project.version(VERSION).download(\"yolov8\", location=str(BASE_PATH))\n",
    "            print(\"  ‚úì SDK report√≥ descarga\")\n",
    "            ok = True\n",
    "        except Exception as e:\n",
    "            print(f\"  ‚ö†Ô∏è  SDK fallo: {e}\")\n",
    "\n",
    "    # 2) Si SDK no deja data.yaml, intentar Universe directo\n",
    "    data_yaml = find_data_yaml_in([BASE_PATH])\n",
    "    if not data_yaml:\n",
    "        urls = build_universe_candidates(WORKSPACE, PROJECT, VERSION, API_KEY, UNIVERSE_URL)\n",
    "        for u in urls:\n",
    "            if try_download_zip(u, BASE_PATH):\n",
    "                ok = True\n",
    "                break\n",
    "\n",
    "    # 3) Buscar data.yaml final\n",
    "    data_yaml = find_data_yaml_in([BASE_PATH])\n",
    "\n",
    "    if data_yaml and data_yaml.exists():\n",
    "        dataset_dir = data_yaml.parent\n",
    "        if 'config' in globals():\n",
    "            config.DATASET_DIR = str(dataset_dir)\n",
    "            config.DATA_YAML = str(data_yaml)\n",
    "        print(\"‚úì Dataset listo\")\n",
    "        print(f\"  DATASET_DIR: {dataset_dir}\")\n",
    "        print(f\"  DATA_YAML:   {data_yaml}\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  data.yaml no encontrado tras la descarga. Estructura actual:\")\n",
    "        def short_ls(p):\n",
    "            p = Path(p)\n",
    "            if not p.exists():\n",
    "                print(f\"  {p} (no existe)\")\n",
    "                return\n",
    "            items = sorted([x.name for x in p.iterdir()])[:80]\n",
    "            print(f\"  {p} -> {items}\")\n",
    "        short_ls(BASE_PATH)\n",
    "        if IN_COLAB:\n",
    "            short_ls(\"/content\")\n",
    "        short_ls(Path.cwd())\n",
    "        print(\"Sugerencias:\")\n",
    "        print(\" - Aseg√∫rate de que ROBOFLOW_API_KEY tenga acceso al proyecto\")\n",
    "        print(\" - Ajusta ROBOFLOW_PROJECT si el slug real es distinto\")\n",
    "        print(\" - Si pegaste la URL de Universe, exporta ROBOFLOW_UNIVERSE_URL con esa URL y reintenta\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error Roboflow: {e}\")\n",
    "    print(\"Verifica API key, workspace y project. Puedes setear ROBOFLOW_API_KEY en el entorno.\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e6f777",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os, sys, json\n",
    "\n",
    "# Determinar ra√≠z de b√∫squeda\n",
    "fallback_base = Path(config.DATASET_DIR) if getattr(config, 'DATASET_DIR', None) else Path(config.BASE_PATH)\n",
    "search_root = fallback_base\n",
    "\n",
    "print(\"search_root:\", search_root)\n",
    "\n",
    "found_yaml = None\n",
    "candidates = []\n",
    "\n",
    "# Buscar data.yaml en ra√≠z y subdirectorios\n",
    "candidate = search_root / \"data.yaml\"\n",
    "if candidate.exists():\n",
    "    found_yaml = candidate\n",
    "else:\n",
    "    for root, dirs, files in os.walk(str(search_root)):\n",
    "        root_p = Path(root)\n",
    "        if \"data.yaml\" in files:\n",
    "            candidates.append(str(root_p))\n",
    "            if not found_yaml:\n",
    "                found_yaml = root_p / \"data.yaml\"\n",
    "        # Heur√≠sticas para detectar ra√≠z YOLO\n",
    "        if (root_p/\"train\"/\"images\").exists():\n",
    "            candidates.append(str(root_p))\n",
    "\n",
    "candidates = sorted(set(candidates))\n",
    "\n",
    "print(\"Posibles ra√≠ces YOLO:\", json.dumps(candidates, indent=2)[:2000])\n",
    "print(\"data.yaml:\", str(found_yaml) if found_yaml else None)\n",
    "\n",
    "# Actualizar config si fuera necesario\n",
    "if found_yaml and (getattr(config, 'DATA_YAML', None) is None or not Path(config.DATA_YAML).exists()):\n",
    "    config.DATA_YAML = str(found_yaml)\n",
    "    config.DATASET_DIR = str(Path(found_yaml).parent)\n",
    "    print(\"‚úì config.DATA_YAML actualizado:\", config.DATA_YAML)\n",
    "    print(\"‚úì config.DATASET_DIR actualizado:\", config.DATASET_DIR)\n",
    "else:\n",
    "    print(\"‚ÑπÔ∏è  Manteniendo valores actuales de config.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "841997d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ac4c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "import time\n",
    "from pathlib import Path\n",
    "class ExperimentLogger:\n",
    "    \"\"\"Sistema de logging profesional para el experimento.\"\"\"\n",
    "\n",
    "    def __init__(self, log_dir: str, experiment_name: str):\n",
    "        self.log_dir = Path(log_dir)\n",
    "        self.log_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # Configurar logging\n",
    "        log_file = self.log_dir / f\"{experiment_name}.log\"\n",
    "\n",
    "        logging.basicConfig(\n",
    "            level=logging.INFO,\n",
    "            format='%(asctime)s | %(levelname)s | %(message)s',\n",
    "            handlers=[\n",
    "                logging.FileHandler(log_file),\n",
    "                logging.StreamHandler(sys.stdout)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        self.start_time = time.time()\n",
    "\n",
    "        self.logger.info(\"=\"*70)\n",
    "        self.logger.info(f\"Experimento iniciado: {experiment_name}\")\n",
    "        self.logger.info(\"=\"*70)\n",
    "\n",
    "    def info(self, message: str):\n",
    "        self.logger.info(message)\n",
    "\n",
    "    def warning(self, message: str):\n",
    "        self.logger.warning(message)\n",
    "\n",
    "    def error(self, message: str):\n",
    "        self.logger.error(message)\n",
    "\n",
    "    def success(self, message: str):\n",
    "        self.logger.info(f\"‚úÖ {message}\")\n",
    "\n",
    "    def section(self, title: str):\n",
    "        self.logger.info(\"\\n\" + \"=\"*70)\n",
    "        self.logger.info(f\"  {title}\")\n",
    "        self.logger.info(\"=\"*70 + \"\\n\")\n",
    "\n",
    "    def elapsed_time(self) -> str:\n",
    "        elapsed = time.time() - self.start_time\n",
    "        hours = int(elapsed // 3600)\n",
    "        minutes = int((elapsed % 3600) // 60)\n",
    "        seconds = int(elapsed % 60)\n",
    "        return f\"{hours:02d}:{minutes:02d}:{seconds:02d}\"\n",
    "\n",
    "    def finalize(self):\n",
    "        self.logger.info(\"=\"*70)\n",
    "        self.logger.info(f\"Experimento finalizado. Tiempo total: {self.elapsed_time()}\")\n",
    "        self.logger.info(\"=\"*70)\n",
    "\n",
    "\n",
    "# Inicializar logger\n",
    "logger = ExperimentLogger(config.LOGS_DIR, config.experiment_name)\n",
    "logger.success(\"Sistema de logging inicializado\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6520802",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "from typing import Dict, Any, Optional, Tuple\n",
    "\n",
    "try:\n",
    "    from tabulate import tabulate\n",
    "except Exception:\n",
    "    tabulate = None\n",
    "\n",
    "try:\n",
    "    from colorama import Fore, Style\n",
    "except Exception:\n",
    "    class _Dummy:\n",
    "        pass\n",
    "    Fore = _Dummy()\n",
    "    Style = _Dummy()\n",
    "    Fore.YELLOW = \"\"\n",
    "    Style.RESET_ALL = \"\"\n",
    "\n",
    "class ResultsManager:\n",
    "    \"\"\"Gesti√≥n centralizada de resultados del experimento.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.results = {}\n",
    "        self.metadata = {\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'config': {\n",
    "                'epochs': config.EPOCHS,\n",
    "                'img_size': config.IMG_SIZE,\n",
    "                'batch_size': config.BATCH_SIZE\n",
    "            }\n",
    "        }\n",
    "\n",
    "    def add_model_results(self, model_name: str, metrics: Dict[str, Any]):\n",
    "        \"\"\"Agregar resultados de un modelo.\"\"\"\n",
    "        self.results[model_name] = metrics\n",
    "        logger.info(f\"Resultados agregados para {model_name}: {metrics}\")\n",
    "\n",
    "    def get_model_results(self, model_name: str) -> Optional[Dict]:\n",
    "        \"\"\"Obtener resultados de un modelo espec√≠fico.\"\"\"\n",
    "        return self.results.get(model_name)\n",
    "\n",
    "    def save_to_json(self, filepath: str):\n",
    "        \"\"\"Guardar resultados en formato JSON.\"\"\"\n",
    "        data = {\n",
    "            'metadata': self.metadata,\n",
    "            'results': self.results\n",
    "        }\n",
    "\n",
    "        with open(filepath, 'w', encoding='utf-8') as f:\n",
    "            json.dump(data, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "        logger.success(f\"Resultados guardados en: {filepath}\")\n",
    "\n",
    "    def load_from_json(self, filepath: str):\n",
    "        \"\"\"Cargar resultados desde JSON.\"\"\"\n",
    "        if os.path.exists(filepath):\n",
    "            with open(filepath, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "                self.metadata = data.get('metadata', {})\n",
    "                self.results = data.get('results', {})\n",
    "            logger.success(f\"Resultados cargados desde: {filepath}\")\n",
    "        else:\n",
    "            logger.warning(f\"No se encontr√≥ el archivo: {filepath}\")\n",
    "\n",
    "    def get_dataframe(self):\n",
    "        \"\"\"Convertir resultados a DataFrame para an√°lisis (si pandas est√° disponible).\"\"\"\n",
    "        if not self.results:\n",
    "            try:\n",
    "                import pandas as pd\n",
    "                return pd.DataFrame()\n",
    "            except Exception:\n",
    "                return None\n",
    "\n",
    "        try:\n",
    "            import pandas as pd\n",
    "        except Exception:\n",
    "            logger.warning(\"pandas no est√° instalado. Omite get_dataframe() o instala pandas.\")\n",
    "            return None\n",
    "\n",
    "        df = pd.DataFrame.from_dict(self.results, orient='index')\n",
    "        df.index.name = 'Modelo'\n",
    "        df = df.reset_index()\n",
    "        return df\n",
    "\n",
    "    def display_summary(self):\n",
    "        \"\"\"Mostrar resumen de resultados.\"\"\"\n",
    "        if not self.results:\n",
    "            print(f\"{Fore.YELLOW}‚ö†Ô∏è  No hay resultados disponibles a√∫n{Style.RESET_ALL}\")\n",
    "            return\n",
    "\n",
    "        df = self.get_dataframe()\n",
    "        if df is None:\n",
    "            print(\"Resumen no disponible (falta pandas).\")\n",
    "            return\n",
    "\n",
    "        print(f\"\\n{Fore.CYAN}{'='*70}\")\n",
    "        print(\"RESUMEN DE RESULTADOS\")\n",
    "        print(f\"{'='*70}{Style.RESET_ALL}\\n\")\n",
    "        if tabulate is not None:\n",
    "            print(tabulate(df, headers='keys', tablefmt='fancy_grid', showindex=False))\n",
    "        else:\n",
    "            print(df)\n",
    "\n",
    "    def get_best_model(self, metric: str = 'mAP50', mode: str = 'max') -> Tuple[str, Any]:\n",
    "        \"\"\"Determinar el mejor modelo seg√∫n una m√©trica.\"\"\"\n",
    "        if not self.results:\n",
    "            return None, None\n",
    "\n",
    "        valid_results = {\n",
    "            name: metrics for name, metrics in self.results.items()\n",
    "            if metric in metrics and isinstance(metrics[metric], (int, float))\n",
    "        }\n",
    "\n",
    "        if not valid_results:\n",
    "            return None, None\n",
    "\n",
    "        if mode == 'max':\n",
    "            best_model = max(valid_results.items(), key=lambda x: x[1][metric])\n",
    "        else:\n",
    "            best_model = min(valid_results.items(), key=lambda x: x[1][metric])\n",
    "\n",
    "        return best_model[0], best_model[1][metric]\n",
    "\n",
    "\n",
    "# Inicializar gestor de resultados\n",
    "results_manager = ResultsManager()\n",
    "logger.success(\"Gestor de resultados inicializado\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "635dafa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funciones auxiliares\n",
    "import os\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import Any, List, Dict\n",
    "\n",
    "import numpy as np\n",
    "import yaml\n",
    "\n",
    "try:\n",
    "    import torch\n",
    "except Exception:\n",
    "    class _TorchFallback:\n",
    "        class _Cuda:\n",
    "            @staticmethod\n",
    "            def is_available():\n",
    "                return False\n",
    "            @staticmethod\n",
    "            def synchronize():\n",
    "                return None\n",
    "        cuda = _Cuda()\n",
    "    torch = _TorchFallback()\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Soporte opcional de MMDetection\n",
    "try:\n",
    "    from mmdet.apis import inference_detector  # type: ignore\n",
    "    MMDET_AVAILABLE = True\n",
    "except Exception:\n",
    "    inference_detector = None  # type: ignore\n",
    "    MMDET_AVAILABLE = False\n",
    "\n",
    "\n",
    "def measure_inference_speed(\n",
    "    model: Any,\n",
    "    sample_images: List[str],\n",
    "    model_type: str = 'yolo',\n",
    "    warmup_runs: int = 3,\n",
    "    num_runs: int = 20\n",
    ") -> Dict[str, float]:\n",
    "    logger.info(f\"Midiendo velocidad de inferencia (warmup={warmup_runs}, runs={num_runs})...\")\n",
    "\n",
    "    if not sample_images:\n",
    "        logger.error(\"No hay im√°genes de muestra disponibles\")\n",
    "        return {'avg_ms': None, 'std_ms': None, 'fps': None}\n",
    "\n",
    "    sample_images = sample_images[:num_runs]\n",
    "    times = []\n",
    "\n",
    "    try:\n",
    "        # Warmup\n",
    "        for _ in range(warmup_runs):\n",
    "            if model_type == 'yolo':\n",
    "                _ = model(sample_images[0], verbose=False)\n",
    "            else:\n",
    "                if MMDET_AVAILABLE and inference_detector is not None:\n",
    "                    _ = inference_detector(model, sample_images[0])\n",
    "                else:\n",
    "                    logger.error(\"inference_detector no disponible. Instala MMDetection o usa model_type='yolo'.\")\n",
    "                    return {'avg_ms': None, 'std_ms': None, 'fps': None}\n",
    "\n",
    "        if hasattr(torch, 'cuda') and torch.cuda.is_available():\n",
    "            torch.cuda.synchronize()\n",
    "\n",
    "        for img_path in tqdm(sample_images, desc=\"Midiendo velocidad\", leave=False):\n",
    "            start = time.time()\n",
    "\n",
    "            if model_type == 'yolo':\n",
    "                _ = model(img_path, verbose=False)\n",
    "            else:\n",
    "                _ = inference_detector(model, img_path)  # type: ignore\n",
    "\n",
    "            if hasattr(torch, 'cuda') and torch.cuda.is_available():\n",
    "                torch.cuda.synchronize()\n",
    "\n",
    "            elapsed = time.time() - start\n",
    "            times.append(elapsed * 1000)\n",
    "\n",
    "        avg_ms = float(np.mean(times))\n",
    "        std_ms = float(np.std(times))\n",
    "        fps = 1000.0 / avg_ms if avg_ms > 0 else 0.0\n",
    "\n",
    "        logger.success(f\"Velocidad medida: {avg_ms:.2f} ¬± {std_ms:.2f} ms/img ({fps:.2f} FPS)\")\n",
    "\n",
    "        return {\n",
    "            'avg_ms': round(avg_ms, 2),\n",
    "            'std_ms': round(std_ms, 2),\n",
    "            'fps': round(fps, 2)\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error midiendo velocidad: {e}\")\n",
    "        return {'avg_ms': None, 'std_ms': None, 'fps': None}\n",
    "\n",
    "\n",
    "def get_model_size_mb(model_path: str) -> float:\n",
    "    if not os.path.exists(model_path):\n",
    "        logger.warning(f\"Modelo no encontrado: {model_path}\")\n",
    "        return None  # type: ignore\n",
    "\n",
    "    size_mb = os.path.getsize(model_path) / (1024 * 1024)\n",
    "    logger.info(f\"Tama√±o del modelo: {size_mb:.2f} MB\")\n",
    "    return round(size_mb, 2)\n",
    "\n",
    "\n",
    "def get_sample_images(dataset_path: str = '', num_samples: int = 20) -> List[str]:\n",
    "    try:\n",
    "        data_yaml_path = getattr(config, 'DATA_YAML', None)\n",
    "        if not data_yaml_path or not Path(data_yaml_path).exists():\n",
    "            logger.error(\"config.DATA_YAML no est√° definido o no existe. Ejecuta la celda de Roboflow.\")\n",
    "            return []\n",
    "\n",
    "        with open(data_yaml_path, 'r') as f:\n",
    "            data_config = yaml.safe_load(f)\n",
    "\n",
    "        base_path = Path(data_yaml_path).parent\n",
    "        val_dir = data_config.get('val') or data_config.get('val_images') or 'images/val'\n",
    "        val_img_dir = base_path / val_dir\n",
    "\n",
    "        image_files = list(val_img_dir.glob('*.jpg')) + list(val_img_dir.glob('*.png'))\n",
    "        sample_images = [str(img) for img in sorted(image_files)[:num_samples]]\n",
    "\n",
    "        logger.info(f\"Se obtuvieron {len(sample_images)} im√°genes de muestra de {val_img_dir}\")\n",
    "        return sample_images\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error obteniendo im√°genes de muestra: {e}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "logger.success(\"Funciones auxiliares cargadas correctamente\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "442c45f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenamiento r√°pido YOLOv8 usando dataset Roboflow\n",
    "from pathlib import Path\n",
    "\n",
    "try:\n",
    "    from ultralytics import YOLO\n",
    "except Exception as e:\n",
    "    logger.error(f\"Ultralytics no disponible: {e}\")\n",
    "    raise\n",
    "\n",
    "if not getattr(config, 'DATA_YAML', None):\n",
    "    logger.error(\"config.DATA_YAML no est√° definido. Ejecuta la celda de Roboflow y la de b√∫squeda de data.yaml.\")\n",
    "else:\n",
    "    model_name = os.environ.get('YOLO_MODEL', 'yolov8n.pt')\n",
    "    device = 0 if (hasattr(torch, 'cuda') and torch.cuda.is_available()) else 'cpu'\n",
    "\n",
    "    logger.section(\"Entrenamiento YOLOv8\")\n",
    "    logger.info(f\"Modelo base: {model_name}\")\n",
    "    logger.info(f\"DATA_YAML: {config.DATA_YAML}\")\n",
    "\n",
    "    model = YOLO(model_name)\n",
    "\n",
    "    results = model.train(\n",
    "        data=config.DATA_YAML,\n",
    "        epochs=config.EPOCHS,\n",
    "        batch=config.BATCH_SIZE,\n",
    "        imgsz=config.IMG_SIZE,\n",
    "        device=device,\n",
    "        project=config.OUTPUTS_DIR,\n",
    "        name=config.experiment_name,\n",
    "        exist_ok=True,\n",
    "        pretrained=True,\n",
    "        seed=42,\n",
    "        deterministic=True,\n",
    "        workers=2,\n",
    "    )\n",
    "\n",
    "    # Localizar mejor modelo\n",
    "    run_dir = None\n",
    "    best_path = None\n",
    "    try:\n",
    "        run_dir = Path(getattr(results, 'save_dir', Path(config.OUTPUTS_DIR) / config.experiment_name))\n",
    "        best_path = run_dir / 'weights' / 'best.pt'\n",
    "    except Exception:\n",
    "        run_dir = Path(config.OUTPUTS_DIR) / config.experiment_name\n",
    "        best_path = run_dir / 'weights' / 'best.pt'\n",
    "\n",
    "    logger.info(f\"Run dir: {run_dir}\")\n",
    "    logger.info(f\"Best weights: {best_path}\")\n",
    "\n",
    "    # Validaci√≥n\n",
    "    try:\n",
    "        val_results = model.val(\n",
    "            data=config.DATA_YAML,\n",
    "            imgsz=config.IMG_SIZE,\n",
    "            batch=config.BATCH_SIZE,\n",
    "            device=device,\n",
    "            workers=2,\n",
    "            verbose=True,\n",
    "        )\n",
    "        metrics = {\n",
    "            'mAP50': float(val_results.box.map50) if hasattr(val_results.box, 'map50') else None,\n",
    "            'mAP50_95': float(val_results.box.map) if hasattr(val_results.box, 'map') else None,\n",
    "            'precision': float(val_results.box.mp) if hasattr(val_results.box, 'mp') else None,\n",
    "            'recall': float(val_results.box.mr) if hasattr(val_results.box, 'mr') else None,\n",
    "        }\n",
    "        results_manager.add_model_results('YOLOv8', metrics)\n",
    "\n",
    "        # Guardar m√©tricas simples\n",
    "        metrics_path = Path(config.OUTPUTS_DIR) / f\"{config.experiment_name}_metrics.json\"\n",
    "        with open(metrics_path, 'w', encoding='utf-8') as f:\n",
    "            import json\n",
    "            json.dump({'metrics': metrics, 'best_model': str(best_path)}, f, indent=2, ensure_ascii=False)\n",
    "        logger.success(f\"Entrenamiento y validaci√≥n completados. M√©tricas: {metrics}\")\n",
    "        logger.success(f\"M√©tricas guardadas en: {metrics_path}\")\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Validaci√≥n no pudo completarse: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad4ab3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import yaml\n",
    "import logging\n",
    "import shutil\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Optional, Union\n",
    "from datetime import datetime\n",
    "import argparse\n",
    "\n",
    "# Imports de YOLO y ML\n",
    "try:\n",
    "    from ultralytics import YOLO\n",
    "    import torch\n",
    "    import torchvision.transforms as transforms\n",
    "    from PIL import Image, ImageDraw, ImageEnhance\n",
    "    import cv2\n",
    "    YOLO_AVAILABLE = True\n",
    "except ImportError as e:\n",
    "    print(f\"‚ö†Ô∏è  Dependencias YOLO no disponibles: {e}\")\n",
    "    print(\"Instalar con: pip install ultralytics torch torchvision pillow opencv-python\")\n",
    "    YOLO_AVAILABLE = False\n",
    "\n",
    "# Configuraci√≥n de logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "    datefmt='%Y-%m-%d %H:%M:%S'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class FruitDatasetManager:\n",
    "    \"\"\"Gestor del dataset de frutas para entrenamiento YOLO.\"\"\"\n",
    "    \n",
    "    def __init__(self, dataset_path: str = \"IA_Etiquetado/Dataset_Frutas\"):\n",
    "        self.dataset_path = Path(dataset_path)\n",
    "        self.images_path = self.dataset_path / \"images\"\n",
    "        self.labels_path = self.dataset_path / \"labels\"\n",
    "        self.yaml_path = self.dataset_path / \"Data.yaml\"\n",
    "        \n",
    "        # Clases de frutas soportadas\n",
    "        self.fruit_classes = {\n",
    "            'apple': 0,\n",
    "            'orange': 1, \n",
    "            'banana': 2,\n",
    "            'grape': 3,\n",
    "            'strawberry': 4,\n",
    "            'pineapple': 5,\n",
    "            'mango': 6,\n",
    "            'watermelon': 7,\n",
    "            'lemon': 8,\n",
    "            'peach': 9\n",
    "        }\n",
    "        \n",
    "        self.setup_directories()\n",
    "        \n",
    "    def setup_directories(self):\n",
    "        \"\"\"Crear estructura de directorios del dataset.\"\"\"\n",
    "        try:\n",
    "            # Crear directorios principales\n",
    "            self.dataset_path.mkdir(exist_ok=True)\n",
    "            \n",
    "            # Crear subdirectorios para train/val/test\n",
    "            for split in ['train', 'val', 'test']:\n",
    "                (self.images_path / split).mkdir(parents=True, exist_ok=True)\n",
    "                (self.labels_path / split).mkdir(parents=True, exist_ok=True)\n",
    "                \n",
    "            logger.info(f\"‚úì Estructura de directorios creada en: {self.dataset_path}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error creando directorios: {e}\")\n",
    "            \n",
    "    def create_dataset_yaml(self, single_class: bool = False, class_name: str = \"football\"):\n",
    "        \"\"\"Crear archivo de configuraci√≥n YAML para YOLO.\"\"\"\n",
    "        \n",
    "        yaml_config = {\n",
    "            'path': str(self.dataset_path.absolute()),\n",
    "            'train': 'images/train',\n",
    "            'val': 'images/val', \n",
    "            'test': 'images/test',\n",
    "        }\n",
    "        if single_class:\n",
    "            yaml_config['nc'] = 1\n",
    "            yaml_config['names'] = [class_name]\n",
    "        else:\n",
    "            yaml_config['nc'] = len(self.fruit_classes)  # n√∫mero de clases\n",
    "            yaml_config['names'] = list(self.fruit_classes.keys())\n",
    "        \n",
    "        try:\n",
    "            with open(self.yaml_path, 'w', encoding='utf-8') as f:\n",
    "                yaml.dump(yaml_config, f, default_flow_style=False, allow_unicode=True)\n",
    "                \n",
    "            logger.info(f\"‚úì Archivo YAML creado: {self.yaml_path}\")\n",
    "            return str(self.yaml_path)\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error creando YAML: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def augment_images(self, input_dir: Path, output_dir: Path, augmentations_per_image: int = 5):\n",
    "        \"\"\"Aplicar data augmentation a las im√°genes.\"\"\"\n",
    "        \n",
    "        if not input_dir.exists():\n",
    "            logger.warning(f\"Directorio no existe: {input_dir}\")\n",
    "            return\n",
    "            \n",
    "        augmentation_transforms = [\n",
    "            self._random_brightness,\n",
    "            self._random_contrast,\n",
    "            self._random_saturation,\n",
    "            self._random_flip,\n",
    "            self._random_rotation,\n",
    "            self._random_scale,\n",
    "            self._add_noise,\n",
    "            self._random_blur\n",
    "        ]\n",
    "        \n",
    "        image_files = list(input_dir.glob(\"*.jpg\")) + list(input_dir.glob(\"*.png\"))\n",
    "        logger.info(f\"Procesando {len(image_files)} im√°genes para augmentation...\")\n",
    "        \n",
    "        for img_path in image_files:\n",
    "            try:\n",
    "                # Cargar imagen original\n",
    "                img = Image.open(img_path)\n",
    "                base_name = img_path.stem\n",
    "                \n",
    "                # Generar versiones aumentadas\n",
    "                for i in range(augmentations_per_image):\n",
    "                    augmented_img = img.copy()\n",
    "                    \n",
    "                    # Aplicar 2-3 transformaciones aleatorias\n",
    "                    num_transforms = random.randint(2, 3)\n",
    "                    selected_transforms = random.sample(augmentation_transforms, num_transforms)\n",
    "                    \n",
    "                    for transform in selected_transforms:\n",
    "                        augmented_img = transform(augmented_img)\n",
    "                    \n",
    "                    # Guardar imagen aumentada\n",
    "                    output_path = output_dir / f\"{base_name}_aug_{i:03d}.jpg\"\n",
    "                    augmented_img.save(output_path, quality=95)\n",
    "                    \n",
    "                    # Copiar/modificar archivo de etiquetas si existe\n",
    "                    self._copy_augmented_labels(img_path, output_path)\n",
    "                    \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error procesando {img_path}: {e}\")\n",
    "                \n",
    "        logger.info(f\"‚úì Data augmentation completado\")\n",
    "    \n",
    "    def _random_brightness(self, img: Image.Image) -> Image.Image:\n",
    "        \"\"\"Ajustar brillo aleatoriamente.\"\"\"\n",
    "        factor = random.uniform(0.7, 1.3)\n",
    "        enhancer = ImageEnhance.Brightness(img)\n",
    "        return enhancer.enhance(factor)\n",
    "    \n",
    "    def _random_contrast(self, img: Image.Image) -> Image.Image:\n",
    "        \"\"\"Ajustar contraste aleatoriamente.\"\"\"\n",
    "        factor = random.uniform(0.8, 1.2)\n",
    "        enhancer = ImageEnhance.Contrast(img)\n",
    "        return enhancer.enhance(factor)\n",
    "    \n",
    "    def _random_saturation(self, img: Image.Image) -> Image.Image:\n",
    "        \"\"\"Ajustar saturaci√≥n aleatoriamente.\"\"\"\n",
    "        factor = random.uniform(0.8, 1.2)\n",
    "        enhancer = ImageEnhance.Color(img)\n",
    "        return enhancer.enhance(factor)\n",
    "    \n",
    "    def _random_flip(self, img: Image.Image) -> Image.Image:\n",
    "        \"\"\"Voltear imagen horizontalmente.\"\"\"\n",
    "        if random.random() > 0.5:\n",
    "            return img.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "        return img\n",
    "    \n",
    "    def _random_rotation(self, img: Image.Image) -> Image.Image:\n",
    "        \"\"\"Rotar imagen ligeramente.\"\"\"\n",
    "        angle = random.uniform(-15, 15)\n",
    "        return img.rotate(angle, expand=False, fillcolor=(255, 255, 255))\n",
    "    \n",
    "    def _random_scale(self, img: Image.Image) -> Image.Image:\n",
    "        \"\"\"Escalar imagen.\"\"\"\n",
    "        scale = random.uniform(0.8, 1.2)\n",
    "        w, h = img.size\n",
    "        new_w, new_h = int(w * scale), int(h * scale)\n",
    "        scaled = img.resize((new_w, new_h), Image.Resampling.LANCZOS)\n",
    "        \n",
    "        # Crop o pad para mantener tama√±o original\n",
    "        if scale > 1.0:\n",
    "            # Crop desde el centro\n",
    "            left = (new_w - w) // 2\n",
    "            top = (new_h - h) // 2\n",
    "            scaled = scaled.crop((left, top, left + w, top + h))\n",
    "        else:\n",
    "            # Pad con blanco\n",
    "            result = Image.new('RGB', (w, h), (255, 255, 255))\n",
    "            paste_x = (w - new_w) // 2\n",
    "            paste_y = (h - new_h) // 2\n",
    "            result.paste(scaled, (paste_x, paste_y))\n",
    "            scaled = result\n",
    "            \n",
    "        return scaled\n",
    "    \n",
    "    def _add_noise(self, img: Image.Image) -> Image.Image:\n",
    "        \"\"\"Agregar ruido gaussiano.\"\"\"\n",
    "        np_img = np.array(img)\n",
    "        noise = np.random.normal(0, 25, np_img.shape).astype(np.uint8)\n",
    "        noisy_img = np.clip(np_img.astype(np.int16) + noise, 0, 255).astype(np.uint8)\n",
    "        return Image.fromarray(noisy_img)\n",
    "    \n",
    "    def _random_blur(self, img: Image.Image) -> Image.Image:\n",
    "        \"\"\"Aplicar desenfoque aleatorio.\"\"\"\n",
    "        if random.random() > 0.7:  # 30% probabilidad\n",
    "            np_img = np.array(img)\n",
    "            kernel_size = random.choice([3, 5])\n",
    "            blurred = cv2.GaussianBlur(np_img, (kernel_size, kernel_size), 0)\n",
    "            return Image.fromarray(blurred)\n",
    "        return img\n",
    "    \n",
    "    def _copy_augmented_labels(self, original_img_path: Path, new_img_path: Path):\n",
    "        \"\"\"Copiar archivo de etiquetas para imagen aumentada.\"\"\"\n",
    "        # Buscar archivo .txt correspondiente\n",
    "        original_label = original_img_path.parent.parent / \"labels\" / original_img_path.parent.name / f\"{original_img_path.stem}.txt\"\n",
    "        \n",
    "        if original_label.exists():\n",
    "            new_label_dir = new_img_path.parent.parent / \"labels\" / new_img_path.parent.name\n",
    "            new_label_dir.mkdir(parents=True, exist_ok=True)\n",
    "            new_label_path = new_label_dir / f\"{new_img_path.stem}.txt\"\n",
    "            \n",
    "            shutil.copy2(original_label, new_label_path)\n",
    "    \n",
    "    def split_dataset(self, train_ratio: float = 0.7, val_ratio: float = 0.2, test_ratio: float = 0.1):\n",
    "        \"\"\"Dividir dataset en train/val/test.\"\"\"\n",
    "        \n",
    "        if abs(train_ratio + val_ratio + test_ratio - 1.0) > 1e-6:\n",
    "            raise ValueError(\"Las proporciones deben sumar 1.0\")\n",
    "        \n",
    "        # Recopilar todas las im√°genes\n",
    "        all_images = []\n",
    "        for ext in ['*.jpg', '*.jpeg', '*.png']:\n",
    "            all_images.extend(self.images_path.glob(ext))\n",
    "        \n",
    "        if not all_images:\n",
    "            logger.warning(\"No se encontraron im√°genes para dividir\")\n",
    "            return\n",
    "        \n",
    "        # Mezclar aleatoriamente\n",
    "        random.shuffle(all_images)\n",
    "        \n",
    "        # Calcular √≠ndices de divisi√≥n\n",
    "        total = len(all_images)\n",
    "        train_end = int(total * train_ratio)\n",
    "        val_end = train_end + int(total * val_ratio)\n",
    "        \n",
    "        # Dividir archivos\n",
    "        train_files = all_images[:train_end]\n",
    "        val_files = all_images[train_end:val_end]\n",
    "        test_files = all_images[val_end:]\n",
    "        \n",
    "        logger.info(f\"Divisi√≥n del dataset:\")\n",
    "        logger.info(f\"  - Entrenamiento: {len(train_files)} im√°genes\")\n",
    "        logger.info(f\"  - Validaci√≥n: {len(val_files)} im√°genes\")\n",
    "        logger.info(f\"  - Prueba: {len(test_files)} im√°genes\")\n",
    "        \n",
    "        # Mover archivos a subdirectorios\n",
    "        self._move_files_to_split('train', train_files)\n",
    "        self._move_files_to_split('val', val_files)\n",
    "        self._move_files_to_split('test', test_files)\n",
    "        \n",
    "    def _move_files_to_split(self, split_name: str, files: List[Path]):\n",
    "        \"\"\"Mover archivos a directorio de divisi√≥n espec√≠fico.\"\"\"\n",
    "        \n",
    "        split_img_dir = self.images_path / split_name\n",
    "        split_label_dir = self.labels_path / split_name\n",
    "        \n",
    "        for img_file in files:\n",
    "            try:\n",
    "                # Mover imagen\n",
    "                new_img_path = split_img_dir / img_file.name\n",
    "                shutil.move(str(img_file), str(new_img_path))\n",
    "                \n",
    "                # Mover etiqueta correspondiente si existe\n",
    "                label_file = self.labels_path / f\"{img_file.stem}.txt\"\n",
    "                if label_file.exists():\n",
    "                    new_label_path = split_label_dir / f\"{img_file.stem}.txt\"\n",
    "                    shutil.move(str(label_file), str(new_label_path))\n",
    "                    \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error moviendo {img_file}: {e}\")\n",
    "    \n",
    "    def validate_dataset(self) -> Dict[str, int]:\n",
    "        \"\"\"Validar integridad del dataset.\"\"\"\n",
    "        \n",
    "        stats = {\n",
    "            'train_images': 0,\n",
    "            'train_labels': 0,\n",
    "            'val_images': 0,\n",
    "            'val_labels': 0,\n",
    "            'test_images': 0,\n",
    "            'test_labels': 0,\n",
    "            'missing_labels': 0,\n",
    "            'empty_labels': 0\n",
    "        }\n",
    "        \n",
    "        for split in ['train', 'val', 'test']:\n",
    "            img_dir = self.images_path / split\n",
    "            label_dir = self.labels_path / split\n",
    "            \n",
    "            # Contar im√°genes\n",
    "            images = list(img_dir.glob(\"*.jpg\")) + list(img_dir.glob(\"*.png\"))\n",
    "            stats[f'{split}_images'] = len(images)\n",
    "            \n",
    "            # Contar y validar etiquetas\n",
    "            labels = list(label_dir.glob(\"*.txt\"))\n",
    "            stats[f'{split}_labels'] = len(labels)\n",
    "            \n",
    "            # Verificar etiquetas faltantes o vac√≠as\n",
    "            for img in images:\n",
    "                label_file = label_dir / f\"{img.stem}.txt\"\n",
    "                if not label_file.exists():\n",
    "                    stats['missing_labels'] += 1\n",
    "                elif label_file.stat().st_size == 0:\n",
    "                    stats['empty_labels'] += 1\n",
    "        \n",
    "        # Mostrar estad√≠sticas\n",
    "        logger.info(\"=== Validaci√≥n del Dataset ===\")\n",
    "        for key, value in stats.items():\n",
    "            logger.info(f\"{key}: {value}\")\n",
    "            \n",
    "        return stats\n",
    "\n",
    "class YOLOv12Trainer:\n",
    "    \"\"\"Entrenador optimizado para modelos YOLOv12.\"\"\"\n",
    "    \n",
    "    def __init__(self, dataset_yaml: str, model_name: str = \"yolov8n.pt\"):\n",
    "        self.dataset_yaml = dataset_yaml\n",
    "        self.model_name = model_name\n",
    "        self.results_dir = Path(\"IA_Etiquetado/Training_Results\")\n",
    "        self.results_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        # Configuraci√≥n de entrenamiento\n",
    "        self.training_config = {\n",
    "            'epochs': 200,\n",
    "            'batch_size': 16,\n",
    "            'image_size': 640,\n",
    "            'learning_rate': 0.01,\n",
    "            'momentum': 0.937,\n",
    "            'weight_decay': 0.0005,\n",
    "            'warmup_epochs': 3,\n",
    "            'patience': 0,  # early stopping\n",
    "            'save_period': 10,  # guardar cada N epochs\n",
    "            'workers': 4,\n",
    "            'device': 'auto',  # auto, cpu, 0, 1, etc.\n",
    "        }\n",
    "        \n",
    "    def setup_training_environment(self):\n",
    "        \"\"\"Configurar ambiente de entrenamiento.\"\"\"\n",
    "        \n",
    "        # Verificar disponibilidad de CUDA\n",
    "        if torch.cuda.is_available():\n",
    "            gpu_count = torch.cuda.device_count()\n",
    "            gpu_name = torch.cuda.get_device_name(0)\n",
    "            logger.info(f\"‚úì CUDA disponible - GPUs: {gpu_count} ({gpu_name})\")\n",
    "            self.training_config['device'] = 0\n",
    "        else:\n",
    "            logger.warning(\"‚ö†Ô∏è  CUDA no disponible - usando CPU\")\n",
    "            self.training_config['device'] = 'cpu'\n",
    "            # Reducir batch size para CPU\n",
    "            self.training_config['batch_size'] = 8\n",
    "            \n",
    "        # Configurar semillas para reproducibilidad\n",
    "        torch.manual_seed(42)\n",
    "        np.random.seed(42)\n",
    "        random.seed(42)\n",
    "        \n",
    "        logger.info(\"‚úì Ambiente de entrenamiento configurado\")\n",
    "    \n",
    "    def train_model(self, custom_config: Optional[Dict] = None) -> str:\n",
    "        \"\"\"Entrenar modelo YOLOv12.\"\"\"\n",
    "        \n",
    "        if not YOLO_AVAILABLE:\n",
    "            raise ImportError(\"Ultralytics YOLO no est√° disponible\")\n",
    "        \n",
    "        # Actualizar configuraci√≥n si se proporciona\n",
    "        if custom_config:\n",
    "            self.training_config.update(custom_config)\n",
    "            \n",
    "        logger.info(\"=== Iniciando Entrenamiento YOLOv12 ===\")\n",
    "        logger.info(f\"Configuraci√≥n: {self.training_config}\")\n",
    "        \n",
    "        try:\n",
    "            # Cargar modelo base\n",
    "            model = YOLO(self.model_name)\n",
    "            logger.info(f\"‚úì Modelo base cargado: {self.model_name}\")\n",
    "            \n",
    "            # Configurar directorio de resultados con timestamp\n",
    "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            run_dir = self.results_dir / f\"train_{timestamp}\"\n",
    "            \n",
    "            # Iniciar entrenamiento\n",
    "            results = model.train(\n",
    "                data=self.dataset_yaml,\n",
    "                epochs=self.training_config['epochs'],\n",
    "                batch=self.training_config['batch_size'],\n",
    "                imgsz=self.training_config['image_size'],\n",
    "                lr0=self.training_config['learning_rate'],\n",
    "                momentum=self.training_config['momentum'],\n",
    "                weight_decay=self.training_config['weight_decay'],\n",
    "                warmup_epochs=self.training_config['warmup_epochs'],\n",
    "                patience=self.training_config['patience'],\n",
    "                save_period=self.training_config['save_period'],\n",
    "                workers=self.training_config['workers'],\n",
    "                device=self.training_config['device'],\n",
    "                project=str(self.results_dir),\n",
    "                name=f\"train_{timestamp}\",\n",
    "                exist_ok=True,\n",
    "                pretrained=True,\n",
    "                optimizer='AdamW',\n",
    "                verbose=True,\n",
    "                seed=42,\n",
    "                deterministic=True,\n",
    "                single_cls=True,\n",
    "                rect=False,  # rectangular training\n",
    "                cos_lr=True,  # cosine learning rate scheduler\n",
    "                close_mosaic=0,  # disable mosaic last N epochs\n",
    "                resume=False,  # resume from last checkpoint\n",
    "                amp=True,  # Automatic Mixed Precision\n",
    "                fraction=1.0,  # dataset fraction to train on\n",
    "                profile=False,  # profile ONNX and TensorRT speeds\n",
    "                freeze=None,  # freeze layers: backbone=10, first3=0:3, etc\n",
    "                # Augmentations\n",
    "                hsv_h=0.0,  # image HSV-Hue augmentation (fraction)\n",
    "                hsv_s=0.0,    # image HSV-Saturation augmentation (fraction)  \n",
    "                hsv_v=0.0,    # image HSV-Value augmentation (fraction)\n",
    "                degrees=0.0,  # image rotation (+/- deg)\n",
    "                translate=0.0, # image translation (+/- fraction)\n",
    "                scale=0.0,    # image scale (+/- gain)\n",
    "                shear=0.0,    # image shear (+/- deg)\n",
    "                perspective=0.0, # image perspective (+/- fraction), range 0-0.001\n",
    "                flipud=0.0,   # image flip up-down (probability)\n",
    "                fliplr=0.0,   # image flip left-right (probability)\n",
    "                mosaic=0.0,   # image mosaic (probability)\n",
    "                mixup=0.0,    # image mixup (probability)\n",
    "                copy_paste=0.0, # segment copy-paste (probability)\n",
    "            )\n",
    "            \n",
    "            # Obtener ruta del mejor modelo\n",
    "            best_model_path = run_dir / \"weights\" / \"best.pt\"\n",
    "            \n",
    "            logger.info(\"=== Entrenamiento Completado ===\")\n",
    "            logger.info(f\"‚úì Mejor modelo guardado en: {best_model_path}\")\n",
    "            \n",
    "            # Guardar m√©tricas de entrenamiento\n",
    "            self._save_training_metrics(results, run_dir)\n",
    "            \n",
    "            return str(best_model_path)\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error durante entrenamiento: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def _save_training_metrics(self, results, run_dir: Path):\n",
    "        \"\"\"Guardar m√©tricas de entrenamiento en JSON.\"\"\"\n",
    "        \n",
    "        try:\n",
    "            metrics = {\n",
    "                'training_completed': datetime.now().isoformat(),\n",
    "                'config': self.training_config,\n",
    "                'dataset': self.dataset_yaml,\n",
    "                'results_dir': str(run_dir),\n",
    "                'best_model': str(run_dir / \"weights\" / \"best.pt\"),\n",
    "                'last_model': str(run_dir / \"weights\" / \"last.pt\"),\n",
    "            }\n",
    "            \n",
    "            # Agregar m√©tricas de resultados si est√°n disponibles\n",
    "            if hasattr(results, 'results_dict'):\n",
    "                metrics['final_metrics'] = results.results_dict\n",
    "            \n",
    "            metrics_file = run_dir / \"training_metrics.json\"\n",
    "            with open(metrics_file, 'w', encoding='utf-8') as f:\n",
    "                json.dump(metrics, f, indent=4, ensure_ascii=False)\n",
    "                \n",
    "            logger.info(f\"‚úì M√©tricas guardadas en: {metrics_file}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error guardando m√©tricas: {e}\")\n",
    "    \n",
    "    def validate_model(self, model_path: str) -> Dict:\n",
    "        \"\"\"Validar modelo entrenado.\"\"\"\n",
    "        \n",
    "        if not Path(model_path).exists():\n",
    "            raise FileNotFoundError(f\"Modelo no encontrado: {model_path}\")\n",
    "        \n",
    "        try:\n",
    "            # Cargar modelo\n",
    "            model = YOLO(model_path)\n",
    "            logger.info(f\"‚úì Modelo cargado para validaci√≥n: {model_path}\")\n",
    "            \n",
    "            # Ejecutar validaci√≥n\n",
    "            results = model.val(\n",
    "                data=self.dataset_yaml,\n",
    "                imgsz=self.training_config['image_size'],\n",
    "                batch=self.training_config['batch_size'],\n",
    "                device=self.training_config['device'],\n",
    "                workers=self.training_config['workers'],\n",
    "                verbose=True,\n",
    "                save_json=True,\n",
    "                save_hybrid=False,\n",
    "                conf=0.001,  # confidence threshold\n",
    "                iou=0.6,     # IoU threshold for NMS\n",
    "                max_det=300, # maximum detections per image\n",
    "                half=True,   # use FP16 half-precision inference\n",
    "                dnn=False,   # use OpenCV DNN for ONNX inference\n",
    "                plots=True,  # save plots and images during validation\n",
    "            )\n",
    "            \n",
    "            # Extraer m√©tricas principales\n",
    "            metrics = {\n",
    "                'mAP50': float(results.box.map50) if hasattr(results.box, 'map50') else 0.0,\n",
    "                'mAP50-95': float(results.box.map) if hasattr(results.box, 'map') else 0.0,\n",
    "                'precision': float(results.box.mp) if hasattr(results.box, 'mp') else 0.0,\n",
    "                'recall': float(results.box.mr) if hasattr(results.box, 'mr') else 0.0,\n",
    "                'validation_completed': datetime.now().isoformat()\n",
    "            }\n",
    "            \n",
    "            logger.info(\"=== Resultados de Validaci√≥n ===\")\n",
    "            for key, value in metrics.items():\n",
    "                if isinstance(value, float):\n",
    "                    logger.info(f\"{key}: {value:.4f}\")\n",
    "                else:\n",
    "                    logger.info(f\"{key}: {value}\")\n",
    "            \n",
    "            return metrics\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error durante validaci√≥n: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def export_model(self, model_path: str, formats: List[str] = ['onnx']) -> Dict[str, str]:\n",
    "        \"\"\"Exportar modelo a diferentes formatos.\"\"\"\n",
    "        \n",
    "        if not Path(model_path).exists():\n",
    "            raise FileNotFoundError(f\"Modelo no encontrado: {model_path}\")\n",
    "        \n",
    "        exported_models = {}\n",
    "        \n",
    "        try:\n",
    "            model = YOLO(model_path)\n",
    "            logger.info(f\"Exportando modelo: {model_path}\")\n",
    "            \n",
    "            for format_name in formats:\n",
    "                try:\n",
    "                    logger.info(f\"Exportando a formato: {format_name}\")\n",
    "                    \n",
    "                    export_path = model.export(\n",
    "                        format=format_name,\n",
    "                        imgsz=self.training_config['image_size'],\n",
    "                        half=True,  # FP16 quantization\n",
    "                        int8=False, # INT8 quantization\n",
    "                        dynamic=False, # dynamic axes\n",
    "                        simplify=True, # simplify ONNX model\n",
    "                        opset=17,   # ONNX opset version\n",
    "                        workspace=4, # TensorRT workspace size (GB)\n",
    "                        nms=True,   # add NMS to model\n",
    "                        lr=0.01,    # learning rate for QAT\n",
    "                        decay=0.0005, # weight decay for QAT\n",
    "                    )\n",
    "                    \n",
    "                    exported_models[format_name] = str(export_path)\n",
    "                    logger.info(f\"‚úì Exportado {format_name}: {export_path}\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Error exportando {format_name}: {e}\")\n",
    "                    \n",
    "            return exported_models\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error durante exportaci√≥n: {e}\")\n",
    "            raise\n",
    "\n",
    "def create_sample_dataset():\n",
    "    \"\"\"Crear dataset de ejemplo con im√°genes sint√©ticas.\"\"\"\n",
    "    \n",
    "    logger.info(\"Creando dataset de ejemplo...\")\n",
    "    \n",
    "    dataset_manager = FruitDatasetManager()\n",
    "    \n",
    "    # Crear algunas im√°genes de ejemplo (simuladas)\n",
    "    sample_dir = dataset_manager.images_path / \"samples\"\n",
    "    sample_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Generar im√°genes sint√©ticas simples para prueba\n",
    "    colors = {\n",
    "        'apple': (255, 0, 0),      # Rojo\n",
    "        'orange': (255, 165, 0),   # Naranja\n",
    "        'banana': (255, 255, 0),   # Amarillo\n",
    "        'grape': (128, 0, 128),    # Morado\n",
    "        'lemon': (255, 255, 100),  # Amarillo lim√≥n\n",
    "    }\n",
    "    \n",
    "    for fruit_name, color in colors.items():\n",
    "        for i in range(10):  # 10 im√°genes por fruta\n",
    "            # Crear imagen simple\n",
    "            img = Image.new('RGB', (640, 640), (255, 255, 255))\n",
    "            draw = ImageDraw.Draw(img)\n",
    "            \n",
    "            # Dibujar c√≠rculo de fruta\n",
    "            center_x = random.randint(100, 540)\n",
    "            center_y = random.randint(100, 540)\n",
    "            radius = random.randint(30, 80)\n",
    "            \n",
    "            draw.ellipse([\n",
    "                center_x - radius, center_y - radius,\n",
    "                center_x + radius, center_y + radius\n",
    "            ], fill=color, outline=(0, 0, 0), width=2)\n",
    "            \n",
    "            # Guardar imagen\n",
    "            img_path = sample_dir / f\"{fruit_name}_{i:03d}.jpg\"\n",
    "            img.save(img_path, quality=95)\n",
    "            \n",
    "            # Crear etiqueta YOLO\n",
    "            label_dir = dataset_manager.labels_path / \"samples\"\n",
    "            label_dir.mkdir(exist_ok=True)\n",
    "            \n",
    "            # Formato YOLO: class_id center_x center_y width height (normalized)\n",
    "            class_id = dataset_manager.fruit_classes[fruit_name]\n",
    "            norm_x = center_x / 640\n",
    "            norm_y = center_y / 640\n",
    "            norm_w = (radius * 2) / 640\n",
    "            norm_h = (radius * 2) / 640\n",
    "            \n",
    "            label_path = label_dir / f\"{fruit_name}_{i:03d}.txt\"\n",
    "            with open(label_path, 'w') as f:\n",
    "                f.write(f\"{class_id} {norm_x:.6f} {norm_y:.6f} {norm_w:.6f} {norm_h:.6f}\\n\")\n",
    "    \n",
    "    logger.info(f\"‚úì Dataset de ejemplo creado con {len(colors) * 10} im√°genes\")\n",
    "    return dataset_manager\n",
    "\n",
    "def main():\n",
    "    \"\"\"Funci√≥n principal de entrenamiento.\"\"\"\n",
    "    \n",
    "    parser = argparse.ArgumentParser(description='Entrenamiento YOLOv12 para VisiFruit')\n",
    "    parser.add_argument('--dataset', type=str, default='IA_Etiquetado/Dataset_Frutas', \n",
    "                       help='Ruta del dataset')\n",
    "    parser.add_argument('--model', type=str, default='yolov8n.pt',\n",
    "                       help='Modelo base a usar')\n",
    "    parser.add_argument('--epochs', type=int, default=200,\n",
    "                       help='N√∫mero de epochs')\n",
    "    parser.add_argument('--batch-size', type=int, default=16,\n",
    "                       help='Batch size')\n",
    "    parser.add_argument('--img-size', type=int, default=640,\n",
    "                       help='Tama√±o de imagen')\n",
    "    parser.add_argument('--create-sample', action='store_true',\n",
    "                       help='Crear dataset de ejemplo')\n",
    "    parser.add_argument('--augment', action='store_true',\n",
    "                       help='Aplicar data augmentation')\n",
    "    parser.add_argument('--validate-only', type=str,\n",
    "                       help='Solo validar modelo existente')\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    try:\n",
    "        if not YOLO_AVAILABLE:\n",
    "            logger.error(\"YOLO no est√° disponible. Instalar dependencias:\")\n",
    "            logger.error(\"pip install ultralytics torch torchvision pillow opencv-python\")\n",
    "            return\n",
    "        \n",
    "        # Crear dataset de ejemplo si se solicita\n",
    "        if args.create_sample:\n",
    "            dataset_manager = create_sample_dataset()\n",
    "        else:\n",
    "            dataset_manager = FruitDatasetManager(args.dataset)\n",
    "        \n",
    "        # Aplicar data augmentation si se solicita\n",
    "        if args.augment:\n",
    "            logger.info(\"Aplicando data augmentation...\")\n",
    "            for split in ['train']:  # Solo en train\n",
    "                input_dir = dataset_manager.images_path / split\n",
    "                dataset_manager.augment_images(input_dir, input_dir, augmentations_per_image=3)\n",
    "        \n",
    "        # Crear configuraci√≥n YAML (una sola clase: football)\n",
    "        yaml_path = dataset_manager.create_dataset_yaml(single_class=True, class_name='football')\n",
    "        if not yaml_path:\n",
    "            logger.error(\"Error creando archivo YAML\")\n",
    "            return\n",
    "        \n",
    "        # Dividir dataset\n",
    "        dataset_manager.split_dataset()\n",
    "        \n",
    "        # Validar dataset\n",
    "        stats = dataset_manager.validate_dataset()\n",
    "        if stats['train_images'] == 0:\n",
    "            logger.error(\"No hay im√°genes de entrenamiento disponibles\")\n",
    "            return\n",
    "        \n",
    "        # Solo validaci√≥n si se especifica\n",
    "        if args.validate_only:\n",
    "            trainer = YOLOv12Trainer(yaml_path, args.model)\n",
    "            trainer.setup_training_environment()\n",
    "            metrics = trainer.validate_model(args.validate_only)\n",
    "            logger.info(f\"Validaci√≥n completada: {metrics}\")\n",
    "            return\n",
    "        \n",
    "        # Entrenar modelo\n",
    "        trainer = YOLOv12Trainer(yaml_path, args.model)\n",
    "        trainer.setup_training_environment()\n",
    "        \n",
    "        # Configuraci√≥n personalizada\n",
    "        custom_config = {\n",
    "            'epochs': args.epochs,\n",
    "            'batch_size': args.batch_size,\n",
    "            'image_size': args.img_size,\n",
    "        }\n",
    "        \n",
    "        # Iniciar entrenamiento\n",
    "        best_model_path = trainer.train_model(custom_config)\n",
    "        \n",
    "        # Validar modelo entrenado\n",
    "        logger.info(\"Validando modelo entrenado...\")\n",
    "        validation_metrics = trainer.validate_model(best_model_path)\n",
    "        \n",
    "        # Exportar modelo\n",
    "        logger.info(\"Exportando modelo...\")\n",
    "        exported_models = trainer.export_model(best_model_path, ['onnx', 'torchscript'])\n",
    "        \n",
    "        logger.info(\"=== Entrenamiento Completado Exitosamente ===\")\n",
    "        logger.info(f\"Mejor modelo: {best_model_path}\")\n",
    "        logger.info(f\"M√©tricas finales: {validation_metrics}\")\n",
    "        logger.info(f\"Modelos exportados: {exported_models}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error durante entrenamiento: {e}\")\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
